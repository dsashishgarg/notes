{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfc83f",
   "metadata": {},
   "source": [
    "\n",
    "# 🕸️ Web Scraping with Python - Beginner's Guide\n",
    "\n",
    "**Created by:** Ashish Garg  \n",
    "**Creation Date:** 10th July 2025  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44847cdd",
   "metadata": {},
   "source": [
    "\n",
    "## 📘 What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of automatically extracting information from websites.  \n",
    "With Python, it's easy to fetch content from web pages and extract specific data using libraries like:\n",
    "\n",
    "- `requests`: To send HTTP requests and receive responses\n",
    "- `BeautifulSoup`: To parse and extract content from HTML/XML documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages (uncomment if not already installed)\n",
    "# !pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9cfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Fetch the webpage using requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code of the response\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "# View the first 500 characters of the HTML content\n",
    "print(response.text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef33534",
   "metadata": {},
   "source": [
    "\n",
    "### 📥 Explanation\n",
    "\n",
    "- `requests.get(url)`: Sends an HTTP GET request to the given URL.\n",
    "- `response.status_code`: HTTP response status (200 = OK).\n",
    "- `response.text`: Contains the raw HTML content of the page.\n",
    "“We start by using the requests library — this sends a request to the webpage just like your browser does when you type a URL.\n",
    "If the page loads correctly, it gives us a status code of 200, which means ‘OK’.\n",
    "If it’s 404, that means ‘page not found’, and 500 means ‘server error’ — good to keep in mind when debugging.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Parse the HTML using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Let's prettify the HTML (for visual understanding)\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b1030",
   "metadata": {},
   "source": [
    "\n",
    "### 🧠 Parsing\n",
    "\n",
    "- `BeautifulSoup(html, \"html.parser\")`: Parses HTML using Python’s built-in HTML parser.\n",
    "- `soup.prettify()`: Returns formatted HTML (easier to read).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a08c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Extract all quotes from the page\n",
    "\n",
    "quotes = soup.find_all(\"span\", class_=\"text\")\n",
    "\n",
    "print(\"Quotes found:\")\n",
    "for quote in quotes:\n",
    "    print(quote.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d4ba4",
   "metadata": {},
   "source": [
    "\n",
    "### 🔍 Extracting Elements\n",
    "\n",
    "- `soup.find_all(tag, class_=...)`: Finds all tags matching the criteria.\n",
    "- `.text`: Extracts inner text from an HTML element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Extract quotes, authors, and tags together\n",
    "\n",
    "for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "    text = quote.find(\"span\", class_=\"text\").text\n",
    "    author = quote.find(\"small\", class_=\"author\").text\n",
    "    tags = [tag.text for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "    print(f\"{text} — {author} [{', '.join(tags)}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2874c3",
   "metadata": {},
   "source": [
    " Save to CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "    text = quote.find(\"span\", class_=\"text\").text\n",
    "    author = quote.find(\"small\", class_=\"author\").text\n",
    "    tags = \", \".join(tag.text for tag in quote.find_all(\"a\", class_=\"tag\"))\n",
    "    data.append([text, author, tags])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Quote\", \"Author\", \"Tags\"])\n",
    "df.to_csv(\"quotes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec6b35",
   "metadata": {},
   "source": [
    "\n",
    "## ✅ Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- Used `requests` to fetch HTML content.\n",
    "- Parsed it using `BeautifulSoup`.\n",
    "- Extracted quotes, authors, and tags from a test website.\n",
    "\n",
    "You can try similar techniques on real-world sites (ethically and legally).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "🕸 Web Scraping Wikipedia IPL Results\n",
    "📅 Created: 10 July 2025\n",
    "👤 Author: Ashish Garg\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the Wikipedia page\n",
    "url = \"https://en.m.wikipedia.org/wiki/List_of_Indian_Premier_League_seasons_and_results\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Locate the first large wikitable (contains season results)\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "# IPL season results is usually the first or second table\n",
    "target_table = tables[0]  # May change in future — check manually if needed\n",
    "\n",
    "# Step 3: Extract headers\n",
    "headers = []\n",
    "for th in target_table.find_all(\"th\"):\n",
    "    headers.append(th.get_text(strip=True))\n",
    "\n",
    "# Clean duplicates and empty headers\n",
    "clean_headers = []\n",
    "seen = set()\n",
    "for h in headers:\n",
    "    if h and h not in seen:\n",
    "        clean_headers.append(h)\n",
    "        seen.add(h)\n",
    "\n",
    "# Step 4: Extract table rows\n",
    "data = []\n",
    "rows = target_table.find_all(\"tr\")[1:]  # Skip header row\n",
    "for row in rows:\n",
    "    cols = row.find_all([\"td\", \"th\"])\n",
    "    cols = [ele.get_text(strip=True).replace('\\xa0', ' ') for ele in cols]\n",
    "    if len(cols) >= 5:\n",
    "        data.append(cols[:len(clean_headers)])  # Trim extra columns if needed\n",
    "\n",
    "# Step 5: Convert to DataFrame and Save to CSV\n",
    "df = pd.DataFrame(data, columns=clean_headers[:len(data[0])])\n",
    "df.to_csv(\"ipl_seasons_results.csv\", index=False)\n",
    "\n",
    "# Display a preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8b591",
   "metadata": {},
   "source": [
    "👋 1. Opening & Greeting (1 min)\n",
    "\n",
    "“Hi everyone! Good [morning/afternoon] and welcome to today’s learning session.\n",
    "I’m really excited to walk you through something fun and practical — Web Scraping using Python.”\n",
    "\n",
    "“Before we jump in, let me introduce myself quickly.”\n",
    "\n",
    "⸻\n",
    "\n",
    "🧑‍💻 2. Self-Introduction (1 min)\n",
    "\n",
    "“I’m Ashish Garg. I love working with data and building small utilities to automate repetitive things — web scraping has always been one of my favorite tools to do that.”\n",
    "\n",
    "“Today, my goal is to show you how anyone, even with very basic Python knowledge, can extract useful data from the internet.”\n",
    "\n",
    "⸻\n",
    "\n",
    "📚 3. What This Session Is About (2 mins)\n",
    "\n",
    "“So, what exactly is web scraping?”\n",
    "“It’s the process of writing code that can visit a website, read its content like a human would, and pick out specific information — automatically.”\n",
    "\n",
    "“In today’s session, we’ll focus on scraping static websites — those that don’t require user interaction or JavaScript rendering.”  “It’s like building your own little robot to surf the web and bring back exactly what you need.”\n",
    "\n",
    "“By the end of the session, you’ll be able to:”\n",
    "\t•\tFetch HTML content of a page\n",
    "\t•\tParse and extract specific data\n",
    "\t•\tSave the results to a CSV file\n",
    "\t•\tAnd yes, you’ll also see how we can apply this on a real-world use case like IPL stats from Wikipedia!”\n",
    "\n",
    "⸻\n",
    "🔍 Slide 3: What is Web Scraping? (3 mins)\n",
    "\n",
    "🗣️ “Web scraping is the process of automatically extracting data from websites using code.”\n",
    "\n",
    "🗣️ “Let’s say you visit a site and see a list of books, or quotes, or match results. What if you wanted to download all of that into a spreadsheet?”\n",
    "\n",
    "🗣️ “Instead of clicking and copying, your script does that — it sends a request to the site, reads the HTML, and picks out the useful data.”\n",
    "\n",
    "🗣️ “This is super useful in things like market analysis, lead generation, trend tracking, even personal projects like compiling cricket stats.”\n",
    "\n",
    "⚙️ 4. Tools & Libraries (2 mins)\n",
    "\n",
    "“For this session, we’ll use just three libraries:”\n",
    "\t•\trequests — to fetch the HTML content of a web page\n",
    "\t•\tBeautifulSoup — to parse and extract data from the HTML\n",
    "\t•\tpandas — to save the extracted data in table form (CSV)\n",
    "\n",
    "“All three are well-documented, popular, and beginner-friendly.”\n",
    "\n",
    "⸻\n",
    "\n",
    "💻 5. Live Code Walkthrough (15 mins)\n",
    "\n",
    "⸻\n",
    "\n",
    "6. Real-World Example: Wikipedia IPL Results (7 mins)\n",
    "\n",
    "“Let’s now take this a step further and scrape a real-world dataset — IPL season results from Wikipedia.”\n",
    "\n",
    "“This page has a structured table, which makes it perfect for scraping.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceba77",
   "metadata": {},
   "source": [
    "url = \"https://en.m.wikipedia.org/wiki/List_of_Indian_Premier_League_seasons_and_results\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "target_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "“We find all tables with class wikitable — Wikipedia uses that for all structured data tables.”\n",
    "# Extract header\n",
    "headers = []\n",
    "for th in target_table.find_all(\"th\"):\n",
    "    headers.append(th.get_text(strip=True))\n",
    "\n",
    "    “Next, we extract the table header and each row’s data.”\n",
    "\n",
    "# Extract rows\n",
    "data = []\n",
    "rows = target_table.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    cols = row.find_all([\"td\", \"th\"])\n",
    "    cols = [col.get_text(strip=True) for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=headers[:len(data[0])])\n",
    "df.to_csv(\"ipl_seasons_results.csv\", index=False)\n",
    "df.head()\n",
    "“Done! We’ve now built a CSV dataset of all IPL finals — programmatically.”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca30e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "“Done! We’ve now built a CSV dataset of all IPL finals — programmatically.”\n",
    "\n",
    "⸻\n",
    "\n",
    "📌 7. Final Notes and Best Practices (1 min)\n",
    "\n",
    "“A few important reminders:”\n",
    "\t•\tAlways check the site’s robots.txt before scraping.\n",
    "\t•\tAvoid scraping sensitive or copyrighted info.\n",
    "\t•\tAdd delays if you’re scraping multiple pages.\n",
    "\t•\tNever overload servers — respect the site.”\n",
    "\n",
    "🎤 8. Wrap-up and Q&A (1–2 mins)\n",
    "\n",
    "“That wraps up the session!\n",
    "I hope this helped demystify web scraping and gave you the confidence to try it out yourself.”\n",
    "\n",
    "“I’m happy to take any questions now!”\n",
    "\n",
    "\n",
    "🗣️ “A few important things before you start building your own scrapers…”\n",
    "\t•\tAlways check the site’s robots.txt\n",
    "\t•\tDon’t send hundreds of requests — that can slow down or even block the site\n",
    "\t•\tAvoid scraping anything behind login pages\n",
    "\t•\tRespect copyright — don’t scrape and reuse data for profit unless permitted\n",
    "\n",
    "🗣️ “And don’t scrape personal data — stay safe and ethical.”\n",
    "\n",
    "⸻\n",
    "\n",
    "🏁 Wrap-up and Takeaways (1 min)\n",
    "\n",
    "🗣️ “That’s the magic of web scraping — a little Python can go a long way!”\n",
    "\n",
    "🗣️ “You now know how to collect real-world data without manual work.”\n",
    "\n",
    "🗣️ “Try this out on your favorite websites. Look for patterns. Build datasets. Have fun!”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc88ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "❓ Q&A (2–3 mins)\n",
    "\n",
    "🗣️ “I’d love to hear your questions — happy to clarify or go deeper into any part!”"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
