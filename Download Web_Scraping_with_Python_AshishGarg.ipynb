{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6bfc83f",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ•¸ï¸ Web Scraping with Python - Beginner's Guide\n",
    "\n",
    "**Created by:** Ashish Garg  \n",
    "**Creation Date:** 10th July 2025  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44847cdd",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ“˜ What is Web Scraping?\n",
    "\n",
    "Web scraping is the process of automatically extracting information from websites.  \n",
    "With Python, it's easy to fetch content from web pages and extract specific data using libraries like:\n",
    "\n",
    "- `requests`: To send HTTP requests and receive responses\n",
    "- `BeautifulSoup`: To parse and extract content from HTML/XML documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c8a0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages (uncomment if not already installed)\n",
    "# !pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8158f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9cfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Fetch the webpage using requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check the status code of the response\n",
    "print(f\"Status Code: {response.status_code}\")\n",
    "\n",
    "# View the first 500 characters of the HTML content\n",
    "print(response.text[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef33534",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ“¥ Explanation\n",
    "\n",
    "- `requests.get(url)`: Sends an HTTP GET request to the given URL.\n",
    "- `response.status_code`: HTTP response status (200 = OK).\n",
    "- `response.text`: Contains the raw HTML content of the page.\n",
    "â€œWe start by using the requests library â€” this sends a request to the webpage just like your browser does when you type a URL.\n",
    "If the page loads correctly, it gives us a status code of 200, which means â€˜OKâ€™.\n",
    "If itâ€™s 404, that means â€˜page not foundâ€™, and 500 means â€˜server errorâ€™ â€” good to keep in mind when debugging.â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d8dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 2: Parse the HTML using BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Let's prettify the HTML (for visual understanding)\n",
    "print(soup.prettify()[:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b1030",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ§  Parsing\n",
    "\n",
    "- `BeautifulSoup(html, \"html.parser\")`: Parses HTML using Pythonâ€™s built-in HTML parser.\n",
    "- `soup.prettify()`: Returns formatted HTML (easier to read).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a08c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Extract all quotes from the page\n",
    "\n",
    "quotes = soup.find_all(\"span\", class_=\"text\")\n",
    "\n",
    "print(\"Quotes found:\")\n",
    "for quote in quotes:\n",
    "    print(quote.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d4ba4",
   "metadata": {},
   "source": [
    "\n",
    "### ğŸ” Extracting Elements\n",
    "\n",
    "- `soup.find_all(tag, class_=...)`: Finds all tags matching the criteria.\n",
    "- `.text`: Extracts inner text from an HTML element.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 4: Extract quotes, authors, and tags together\n",
    "\n",
    "for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "    text = quote.find(\"span\", class_=\"text\").text\n",
    "    author = quote.find(\"small\", class_=\"author\").text\n",
    "    tags = [tag.text for tag in quote.find_all(\"a\", class_=\"tag\")]\n",
    "    print(f\"{text} â€” {author} [{', '.join(tags)}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2874c3",
   "metadata": {},
   "source": [
    " Save to CSV (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf2ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for quote in soup.find_all(\"div\", class_=\"quote\"):\n",
    "    text = quote.find(\"span\", class_=\"text\").text\n",
    "    author = quote.find(\"small\", class_=\"author\").text\n",
    "    tags = \", \".join(tag.text for tag in quote.find_all(\"a\", class_=\"tag\"))\n",
    "    data.append([text, author, tags])\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"Quote\", \"Author\", \"Tags\"])\n",
    "df.to_csv(\"quotes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ec6b35",
   "metadata": {},
   "source": [
    "\n",
    "## âœ… Summary\n",
    "\n",
    "In this notebook, we:\n",
    "- Used `requests` to fetch HTML content.\n",
    "- Parsed it using `BeautifulSoup`.\n",
    "- Extracted quotes, authors, and tags from a test website.\n",
    "\n",
    "You can try similar techniques on real-world sites (ethically and legally).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a95b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ğŸ•¸ Web Scraping Wikipedia IPL Results\n",
    "ğŸ“… Created: 10 July 2025\n",
    "ğŸ‘¤ Author: Ashish Garg\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Fetch the Wikipedia page\n",
    "url = \"https://en.m.wikipedia.org/wiki/List_of_Indian_Premier_League_seasons_and_results\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Step 2: Locate the first large wikitable (contains season results)\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "\n",
    "# IPL season results is usually the first or second table\n",
    "target_table = tables[0]  # May change in future â€” check manually if needed\n",
    "\n",
    "# Step 3: Extract headers\n",
    "headers = []\n",
    "for th in target_table.find_all(\"th\"):\n",
    "    headers.append(th.get_text(strip=True))\n",
    "\n",
    "# Clean duplicates and empty headers\n",
    "clean_headers = []\n",
    "seen = set()\n",
    "for h in headers:\n",
    "    if h and h not in seen:\n",
    "        clean_headers.append(h)\n",
    "        seen.add(h)\n",
    "\n",
    "# Step 4: Extract table rows\n",
    "data = []\n",
    "rows = target_table.find_all(\"tr\")[1:]  # Skip header row\n",
    "for row in rows:\n",
    "    cols = row.find_all([\"td\", \"th\"])\n",
    "    cols = [ele.get_text(strip=True).replace('\\xa0', ' ') for ele in cols]\n",
    "    if len(cols) >= 5:\n",
    "        data.append(cols[:len(clean_headers)])  # Trim extra columns if needed\n",
    "\n",
    "# Step 5: Convert to DataFrame and Save to CSV\n",
    "df = pd.DataFrame(data, columns=clean_headers[:len(data[0])])\n",
    "df.to_csv(\"ipl_seasons_results.csv\", index=False)\n",
    "\n",
    "# Display a preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8b591",
   "metadata": {},
   "source": [
    "ğŸ‘‹ 1. Opening & Greeting (1 min)\n",
    "\n",
    "â€œHi everyone! Good [morning/afternoon] and welcome to todayâ€™s learning session.\n",
    "Iâ€™m really excited to walk you through something fun and practical â€” Web Scraping using Python.â€\n",
    "\n",
    "â€œBefore we jump in, let me introduce myself quickly.â€\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ§‘â€ğŸ’» 2. Self-Introduction (1 min)\n",
    "\n",
    "â€œIâ€™m Ashish Garg. I love working with data and building small utilities to automate repetitive things â€” web scraping has always been one of my favorite tools to do that.â€\n",
    "\n",
    "â€œToday, my goal is to show you how anyone, even with very basic Python knowledge, can extract useful data from the internet.â€\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ“š 3. What This Session Is About (2 mins)\n",
    "\n",
    "â€œSo, what exactly is web scraping?â€\n",
    "â€œItâ€™s the process of writing code that can visit a website, read its content like a human would, and pick out specific information â€” automatically.â€\n",
    "\n",
    "â€œIn todayâ€™s session, weâ€™ll focus on scraping static websites â€” those that donâ€™t require user interaction or JavaScript rendering.â€  â€œItâ€™s like building your own little robot to surf the web and bring back exactly what you need.â€\n",
    "\n",
    "â€œBy the end of the session, youâ€™ll be able to:â€\n",
    "\tâ€¢\tFetch HTML content of a page\n",
    "\tâ€¢\tParse and extract specific data\n",
    "\tâ€¢\tSave the results to a CSV file\n",
    "\tâ€¢\tAnd yes, youâ€™ll also see how we can apply this on a real-world use case like IPL stats from Wikipedia!â€\n",
    "\n",
    "â¸»\n",
    "ğŸ” Slide 3: What is Web Scraping? (3 mins)\n",
    "\n",
    "ğŸ—£ï¸ â€œWeb scraping is the process of automatically extracting data from websites using code.â€\n",
    "\n",
    "ğŸ—£ï¸ â€œLetâ€™s say you visit a site and see a list of books, or quotes, or match results. What if you wanted to download all of that into a spreadsheet?â€\n",
    "\n",
    "ğŸ—£ï¸ â€œInstead of clicking and copying, your script does that â€” it sends a request to the site, reads the HTML, and picks out the useful data.â€\n",
    "\n",
    "ğŸ—£ï¸ â€œThis is super useful in things like market analysis, lead generation, trend tracking, even personal projects like compiling cricket stats.â€\n",
    "\n",
    "âš™ï¸ 4. Tools & Libraries (2 mins)\n",
    "\n",
    "â€œFor this session, weâ€™ll use just three libraries:â€\n",
    "\tâ€¢\trequests â€” to fetch the HTML content of a web page\n",
    "\tâ€¢\tBeautifulSoup â€” to parse and extract data from the HTML\n",
    "\tâ€¢\tpandas â€” to save the extracted data in table form (CSV)\n",
    "\n",
    "â€œAll three are well-documented, popular, and beginner-friendly.â€\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ’» 5. Live Code Walkthrough (15 mins)\n",
    "\n",
    "â¸»\n",
    "\n",
    "6. Real-World Example: Wikipedia IPL Results (7 mins)\n",
    "\n",
    "â€œLetâ€™s now take this a step further and scrape a real-world dataset â€” IPL season results from Wikipedia.â€\n",
    "\n",
    "â€œThis page has a structured table, which makes it perfect for scraping.â€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dceba77",
   "metadata": {},
   "source": [
    "url = \"https://en.m.wikipedia.org/wiki/List_of_Indian_Premier_League_seasons_and_results\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "tables = soup.find_all(\"table\", class_=\"wikitable\")\n",
    "target_table = tables[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81adbb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "â€œWe find all tables with class wikitable â€” Wikipedia uses that for all structured data tables.â€\n",
    "# Extract header\n",
    "headers = []\n",
    "for th in target_table.find_all(\"th\"):\n",
    "    headers.append(th.get_text(strip=True))\n",
    "\n",
    "    â€œNext, we extract the table header and each rowâ€™s data.â€\n",
    "\n",
    "# Extract rows\n",
    "data = []\n",
    "rows = target_table.find_all(\"tr\")[1:]\n",
    "for row in rows:\n",
    "    cols = row.find_all([\"td\", \"th\"])\n",
    "    cols = [col.get_text(strip=True) for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=headers[:len(data[0])])\n",
    "df.to_csv(\"ipl_seasons_results.csv\", index=False)\n",
    "df.head()\n",
    "â€œDone! Weâ€™ve now built a CSV dataset of all IPL finals â€” programmatically.â€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca30e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "â€œDone! Weâ€™ve now built a CSV dataset of all IPL finals â€” programmatically.â€\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ“Œ 7. Final Notes and Best Practices (1 min)\n",
    "\n",
    "â€œA few important reminders:â€\n",
    "\tâ€¢\tAlways check the siteâ€™s robots.txt before scraping.\n",
    "\tâ€¢\tAvoid scraping sensitive or copyrighted info.\n",
    "\tâ€¢\tAdd delays if youâ€™re scraping multiple pages.\n",
    "\tâ€¢\tNever overload servers â€” respect the site.â€\n",
    "\n",
    "ğŸ¤ 8. Wrap-up and Q&A (1â€“2 mins)\n",
    "\n",
    "â€œThat wraps up the session!\n",
    "I hope this helped demystify web scraping and gave you the confidence to try it out yourself.â€\n",
    "\n",
    "â€œIâ€™m happy to take any questions now!â€\n",
    "\n",
    "\n",
    "ğŸ—£ï¸ â€œA few important things before you start building your own scrapersâ€¦â€\n",
    "\tâ€¢\tAlways check the siteâ€™s robots.txt\n",
    "\tâ€¢\tDonâ€™t send hundreds of requests â€” that can slow down or even block the site\n",
    "\tâ€¢\tAvoid scraping anything behind login pages\n",
    "\tâ€¢\tRespect copyright â€” donâ€™t scrape and reuse data for profit unless permitted\n",
    "\n",
    "ğŸ—£ï¸ â€œAnd donâ€™t scrape personal data â€” stay safe and ethical.â€\n",
    "\n",
    "â¸»\n",
    "\n",
    "ğŸ Wrap-up and Takeaways (1 min)\n",
    "\n",
    "ğŸ—£ï¸ â€œThatâ€™s the magic of web scraping â€” a little Python can go a long way!â€\n",
    "\n",
    "ğŸ—£ï¸ â€œYou now know how to collect real-world data without manual work.â€\n",
    "\n",
    "ğŸ—£ï¸ â€œTry this out on your favorite websites. Look for patterns. Build datasets. Have fun!â€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc88ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "â“ Q&A (2â€“3 mins)\n",
    "\n",
    "ğŸ—£ï¸ â€œIâ€™d love to hear your questions â€” happy to clarify or go deeper into any part!â€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
